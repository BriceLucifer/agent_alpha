import asyncio
import json
import sys
from typing import Optional
from contextlib import AsyncExitStack

# MCP å®¢æˆ·ç«¯ç›¸å…³å¯¼å…¥
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from openai import OpenAI

# ç¯å¢ƒå˜é‡åŠ è½½ç›¸å…³
from dotenv import load_dotenv
import os

# æœ¬åœ°æœåŠ¡å¯¼å…¥
import tts
import stt
from wake_word import WakeWordDetector
from knowledge_base import KnowledgeBase

load_dotenv()

class MCPClient:
    """
    ä½¿ç”¨ OpenAI æ ¼å¼ API çš„ MCP å®¢æˆ·ç«¯ç±»
    å¤„ç† MCP æœåŠ¡å™¨è¿æ¥å’Œ OpenAI å…¼å®¹ API çš„äº¤äº’
    é›†æˆæœ¬åœ° TTS å’Œ STT åŠŸèƒ½
    """
    
    def __init__(self):
        # MCP å®¢æˆ·ç«¯ä¼šè¯
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        # ä½¿ç”¨ DeepSeek API é…ç½®ï¼ˆåˆ é™¤é‡å¤é…ç½®ï¼‰
        self.llm_client = OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url=os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        )
        self.model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        
        # æœ¬åœ°æœåŠ¡åˆå§‹åŒ–
        self.stt_service = stt.SpeechToText()
        self.wake_detector = WakeWordDetector()
        self.knowledge_base = KnowledgeBase()
        
        # å¯¹è¯å†å²
        self.conversation_history = []
        
        # è®¾ç½®å”¤é†’å›è°ƒ
        self.wake_detector.set_wake_callback(self.handle_wake_word)
    
    async def connect_to_server(self, server_script_path: str):
        """è¿æ¥åˆ°MCPæœåŠ¡"""
        is_python = server_script_path.endswith('.py')
        is_js = server_script_path.endswith('.js')
        
        if not (is_python or is_js):
            raise ValueError("æœåŠ¡å™¨è„šæœ¬å¿…é¡»æ˜¯ .py æˆ– .js æ–‡ä»¶")
        
        command = "python" if is_python else "node"
        server_params = StdioServerParameters(
            command=command,
            args=[server_script_path],
            env=None
        )
        
        # å»ºç«‹è¿æ¥
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        
        # åˆ›å»ºä¼šè¯
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(stdio_transport[0], stdio_transport[1])
        )
        
        # åˆå§‹åŒ–ä¼šè¯
        await self.session.initialize()
        
        print(f"âœ… å·²è¿æ¥åˆ° MCP æœåŠ¡: {server_script_path}")
    
    async def handle_wake_word(self):
        """å¤„ç†å”¤é†’è¯æ£€æµ‹"""
        print("ğŸ¤ æ£€æµ‹åˆ°å”¤é†’è¯ï¼Œå¼€å§‹ç›‘å¬...")
        
        # æ’­æ”¾æç¤ºéŸ³
        await tts.tts_play("æˆ‘åœ¨å¬ï¼Œè¯·è¯´è¯")
        
        # å¼€å§‹è¯­éŸ³è¯†åˆ«
        user_input = await self.stt_service.listen_and_recognize(timeout=10.0)
        
        if user_input:
            print(f"ğŸ‘¤ ç”¨æˆ·è¯´: {user_input}")
            
            # å¤„ç†ç”¨æˆ·æŸ¥è¯¢
            response = await self.process_query(user_input)
            
            # æ’­æ”¾å›ç­”
            if response:
                await tts.tts_play(response)
        else:
            await tts.tts_play("æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¬æ¸…æ¥š")
    
    async def process_query(self, query: str) -> str:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": query})
            
            # æ£€æŸ¥çŸ¥è¯†åº“
            kb_results = await self.knowledge_base.search_documents(query, top_k=3)
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚"
            if kb_results:
                kb_context = "\n".join([doc["content"] for doc in kb_results])
                system_prompt += f"\n\nç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯:\n{kb_context}"
            
            # è·å–å¯ç”¨å·¥å…·
            tools = []
            if self.session:
                tools_result = await self.session.list_tools()
                tools = tools_result.tools
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt},
                *self.conversation_history[-10:]  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            ]
            
            # å¦‚æœæœ‰å·¥å…·ï¼Œä½¿ç”¨å·¥å…·è°ƒç”¨
            if tools:
                # è½¬æ¢å·¥å…·æ ¼å¼ä¸ºOpenAIæ ¼å¼
                openai_tools = self._convert_tools_to_openai_format(tools)
                
                response = self.llm_client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    tools=openai_tools,
                    tool_choice="auto"
                )
                
                message = response.choices[0].message
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                if message.tool_calls:
                    return await self._handle_tool_calls(message, messages)
                else:
                    final_response = message.content
            else:
                # ç›´æ¥å¯¹è¯
                response = self.llm_client.chat.completions.create(
                    model=self.model,
                    messages=messages
                )
                final_response = response.choices[0].message.content
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.conversation_history.append({"role": "assistant", "content": final_response})
            
            return final_response
            
        except Exception as e:
            print(f"âŒ å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
    
    def _convert_tools_to_openai_format(self, mcp_tools):
        """å°†MCPå·¥å…·æ ¼å¼è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        openai_tools = []
        for tool in mcp_tools:
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            }
            openai_tools.append(openai_tool)
        return openai_tools
    
    async def _handle_tool_calls(self, message, messages):
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
        messages.append({
            "role": "assistant",
            "content": message.content,
            "tool_calls": [{
                "id": call.id,
                "type": "function",
                "function": {
                    "name": call.function.name,
                    "arguments": call.function.arguments
                }
            } for call in message.tool_calls]
        })
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        for tool_call in message.tool_calls:
            try:
                # è§£æå‚æ•°
                args = json.loads(tool_call.function.arguments)
                
                # è°ƒç”¨MCPå·¥å…·
                result = await self.session.call_tool(
                    tool_call.function.name,
                    args
                )
                
                # æ·»åŠ å·¥å…·ç»“æœ
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(result.content)
                })
                
            except Exception as e:
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"
                })
        
        # è·å–æœ€ç»ˆå›å¤
        final_response = self.llm_client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        
        return final_response.choices[0].message.content
    
    async def start_voice_interaction(self):
        """å¯åŠ¨è¯­éŸ³äº¤äº’æ¨¡å¼"""
        print("ğŸ™ï¸ å¯åŠ¨è¯­éŸ³äº¤äº’æ¨¡å¼...")
        print("ğŸ’¡ è¯´å‡ºå”¤é†’è¯å¼€å§‹å¯¹è¯ï¼ŒæŒ‰ Ctrl+C é€€å‡º")
        
        # å¯åŠ¨å”¤é†’è¯æ£€æµ‹
        self.wake_detector.start_listening()
        
        try:
            # ä¿æŒè¿è¡Œ
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ é€€å‡ºè¯­éŸ³äº¤äº’æ¨¡å¼")
        finally:
            self.wake_detector.stop_listening()
    
    async def text_interaction(self):
        """æ–‡æœ¬äº¤äº’æ¨¡å¼"""
        print("ğŸ’¬ å¯åŠ¨æ–‡æœ¬äº¤äº’æ¨¡å¼...")
        print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break
                
                if not user_input:
                    continue
                
                print("ğŸ¤– æ€è€ƒä¸­...")
                response = await self.process_query(user_input)
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
        
        print("ğŸ‘‹ å†è§ï¼")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.exit_stack.aclose()

# ä¸»å‡½æ•°
async def func_call_main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python fun_call.py <mcp_server_script> [mode]")
        print("mode: voice (è¯­éŸ³æ¨¡å¼) æˆ– text (æ–‡æœ¬æ¨¡å¼ï¼Œé»˜è®¤)")
        return
    
    server_script = sys.argv[1]
    mode = sys.argv[2] if len(sys.argv) > 2 else "text"
    
    client = MCPClient()
    
    try:
        # è¿æ¥åˆ°MCPæœåŠ¡
        await client.connect_to_server(server_script)
        
        # æ ¹æ®æ¨¡å¼å¯åŠ¨äº¤äº’
        if mode.lower() == "voice":
            await client.start_voice_interaction()
        else:
            await client.text_interaction()
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(func_call_main())
