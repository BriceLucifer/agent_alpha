import json
import pyaudio
# import vosk  # ç§»é™¤è¿™è¡Œ
import asyncio
# import whisper  # ç§»é™¤è¿™è¡Œï¼Œä½¿ç”¨ faster-whisper
from faster_whisper import WhisperModel
import numpy as np
import sounddevice as sd
import tempfile
import asyncio
from typing import Optional, Callable
import logging
import os
import tempfile
import wave
from pathlib import Path

class SpeechToText:
    """
    åŸºäº Vosk çš„ç¦»çº¿è¯­éŸ³è½¬æ–‡å­—æœåŠ¡
    å®Œå…¨å…¼å®¹åŸæœ‰æ¥å£ï¼Œé€‚åˆ Jetson NX ç­‰èµ„æºå—é™è®¾å¤‡
    """
    
    def __init__(self, model_path: str = "vosk-model-cn-0.22", language: str = "zh-CN"):
        """
        åˆå§‹åŒ– Vosk STT æœåŠ¡
        
        Args:
            model_path: Vosk æ¨¡å‹è·¯å¾„
            language: è¯­è¨€ä»£ç ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        """
        self.language = language
        self.sample_rate = 16000
        self.logger = logging.getLogger(__name__)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            self.logger.error(f"Vosk æ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
            self.logger.info("è¯·ä¸‹è½½æ¨¡å‹: wget https://alphacephei.com/vosk/models/vosk-model-cn-0.22.zip && unzip vosk-model-cn-0.22.zip")
            raise FileNotFoundError(f"Vosk æ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
        
        # åˆå§‹åŒ– Vosk æ¨¡å‹å’Œè¯†åˆ«å™¨
        try:
            self.model = vosk.Model(model_path)
            self.recognizer = vosk.KaldiRecognizer(self.model, self.sample_rate)
            self.logger.info(f"Vosk STT åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {model_path}")
        except Exception as e:
            self.logger.error(f"Vosk åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # éŸ³é¢‘é…ç½®
        self.chunk_size = 4096
        self.format = pyaudio.paInt16
        self.channels = 1
        
        # è¯†åˆ«å™¨å‚æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        self.energy_threshold = 300
        self.dynamic_energy_threshold = True
        self.pause_threshold = 0.8
        self.phrase_threshold = 0.3
    
    async def listen_and_recognize(self, timeout: float = 5.0, phrase_time_limit: float = 10.0) -> Optional[str]:
        """
        ç›‘å¬éº¦å…‹é£å¹¶è¯†åˆ«è¯­éŸ³ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
        
        Args:
            timeout: ç­‰å¾…è¯­éŸ³å¼€å§‹çš„è¶…æ—¶æ—¶é—´
            phrase_time_limit: å•æ¬¡å½•éŸ³çš„æœ€å¤§æ—¶é•¿
        
        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            self.logger.info("å¼€å§‹ç›‘å¬è¯­éŸ³...")
            
            def _listen_and_recognize():
                p = pyaudio.PyAudio()
                stream = p.open(
                    format=self.format,
                    channels=self.channels,
                    rate=self.sample_rate,
                    input=True,
                    frames_per_buffer=self.chunk_size
                )
                
                # é‡ç½®è¯†åˆ«å™¨
                self.recognizer = vosk.KaldiRecognizer(self.model, self.sample_rate)
                
                frames_count = int(self.sample_rate / self.chunk_size * phrase_time_limit)
                speech_detected = False
                
                try:
                    for i in range(frames_count):
                        data = stream.read(self.chunk_size, exception_on_overflow=False)
                        
                        if self.recognizer.AcceptWaveform(data):
                            result = json.loads(self.recognizer.Result())
                            text = result.get('text', '').strip()
                            if text:
                                speech_detected = True
                                return text
                    
                    # è·å–æœ€ç»ˆç»“æœ
                    if not speech_detected:
                        final_result = json.loads(self.recognizer.FinalResult())
                        text = final_result.get('text', '').strip()
                        return text if text else None
                    
                finally:
                    stream.stop_stream()
                    stream.close()
                    p.terminate()
                    
                return None
            
            result = await asyncio.to_thread(_listen_and_recognize)
            
            if result:
                self.logger.info(f"è¯†åˆ«ç»“æœ: {result}")
                return result
            else:
                self.logger.warning("æœªæ£€æµ‹åˆ°è¯­éŸ³")
                return None
                
        except Exception as e:
            self.logger.error(f"è¯­éŸ³ç›‘å¬å¤±è´¥: {e}")
            return None
    
    async def recognize_file(self, file_path: str) -> Optional[str]:
        """
        è¯†åˆ«éŸ³é¢‘æ–‡ä»¶ä¸­çš„è¯­éŸ³ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            def _recognize_file():
                wf = wave.open(file_path, 'rb')
                
                # æ£€æŸ¥éŸ³é¢‘æ ¼å¼
                if (wf.getnchannels() != 1 or 
                    wf.getsampwidth() != 2 or 
                    wf.getframerate() != self.sample_rate):
                    self.logger.error(
                        f"éŸ³é¢‘æ ¼å¼ä¸æ”¯æŒ: {wf.getnchannels()}å£°é“, "
                        f"{wf.getsampwidth()*8}ä½, {wf.getframerate()}Hz. "
                        f"éœ€è¦: 1å£°é“, 16ä½, {self.sample_rate}Hz"
                    )
                    wf.close()
                    return None
                
                # é‡ç½®è¯†åˆ«å™¨
                recognizer = vosk.KaldiRecognizer(self.model, self.sample_rate)
                results = []
                
                try:
                    while True:
                        data = wf.readframes(self.chunk_size)
                        if len(data) == 0:
                            break
                        
                        if recognizer.AcceptWaveform(data):
                            result = json.loads(recognizer.Result())
                            text = result.get('text', '').strip()
                            if text:
                                results.append(text)
                    
                    # è·å–æœ€ç»ˆç»“æœ
                    final_result = json.loads(recognizer.FinalResult())
                    final_text = final_result.get('text', '').strip()
                    if final_text:
                        results.append(final_text)
                    
                finally:
                    wf.close()
                
                full_text = ' '.join(results).strip()
                return full_text if full_text else None
            
            result = await asyncio.to_thread(_recognize_file)
            
            if result:
                self.logger.info(f"æ–‡ä»¶è¯†åˆ«ç»“æœ: {result}")
            else:
                self.logger.warning("æ–‡ä»¶è¯†åˆ«å¤±è´¥")
            
            return result
            
        except Exception as e:
            self.logger.error(f"éŸ³é¢‘æ–‡ä»¶è¯†åˆ«å¤±è´¥: {e}")
            return None
    
    async def record_audio(self, duration: float = 5.0, save_path: Optional[str] = None) -> str:
        """
        å½•åˆ¶éŸ³é¢‘åˆ°æ–‡ä»¶ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
        
        Args:
            duration: å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        
        Returns:
            str: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        if save_path is None:
            save_path = tempfile.mktemp(suffix=".wav")
        
        def _record():
            chunk = 1024
            format = pyaudio.paInt16
            channels = 1
            rate = self.sample_rate
            
            p = pyaudio.PyAudio()
            
            stream = p.open(
                format=format,
                channels=channels,
                rate=rate,
                input=True,
                frames_per_buffer=chunk
            )
            
            frames = []
            
            for _ in range(0, int(rate / chunk * duration)):
                data = stream.read(chunk)
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            wf = wave.open(save_path, 'wb')
            wf.setnchannels(channels)
            wf.setsampwidth(p.get_sample_size(format))
            wf.setframerate(rate)
            wf.writeframes(b''.join(frames))
            wf.close()
            
            return save_path
        
        file_path = await asyncio.to_thread(_record)
        self.logger.info(f"éŸ³é¢‘å·²ä¿å­˜åˆ°: {file_path}")
        return file_path
    
    async def start_streaming(self, callback: Callable[[str], None]) -> tuple:
        """
        å¯åŠ¨æµå¼è¯­éŸ³è¯†åˆ«
        
        Args:
            callback: è¯†åˆ«ç»“æœå›è°ƒå‡½æ•°
        
        Returns:
            (stream, pyaudio_instance): ç”¨äºåç»­å…³é—­
        """
        p = pyaudio.PyAudio()
        
        # é‡ç½®è¯†åˆ«å™¨
        self.recognizer = vosk.KaldiRecognizer(self.model, self.sample_rate)
        
        def audio_callback(in_data, frame_count, time_info, status):
            try:
                if self.recognizer.AcceptWaveform(in_data):
                    result = json.loads(self.recognizer.Result())
                    text = result.get('text', '').strip()
                    if text:
                        # å¼‚æ­¥æ‰§è¡Œå›è°ƒ
                        asyncio.create_task(self._async_callback(callback, text))
            except Exception as e:
                self.logger.error(f"æµå¼è¯†åˆ«é”™è¯¯: {e}")
            
            return (None, pyaudio.paContinue)
        
        stream = p.open(
            format=self.format,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=audio_callback
        )
        
        stream.start_stream()
        self.logger.info("æµå¼è¯†åˆ«å·²å¯åŠ¨")
        
        return stream, p
    
    async def _async_callback(self, callback: Callable[[str], None], text: str):
        """å¼‚æ­¥æ‰§è¡Œå›è°ƒå‡½æ•°"""
        try:
            if asyncio.iscoroutinefunction(callback):
                await callback(text)
            else:
                await asyncio.to_thread(callback, text)
        except Exception as e:
            self.logger.error(f"å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    def stop_streaming(self, stream, p):
        """åœæ­¢æµå¼è¯†åˆ«"""
        try:
            if stream.is_active():
                stream.stop_stream()
            stream.close()
            p.terminate()
            self.logger.info("æµå¼è¯†åˆ«å·²åœæ­¢")
        except Exception as e:
            self.logger.error(f"åœæ­¢æµå¼è¯†åˆ«å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    try:
        # åˆå§‹åŒ–ï¼ˆç¡®ä¿å·²ä¸‹è½½æ¨¡å‹ï¼‰
        stt = SpeechToText(model_path="vosk-model-cn-0.22")
        
        # å•æ¬¡è¯†åˆ«
        print("è¯·è¯´è¯ï¼ˆ5ç§’å†…ï¼‰...")
        text = await stt.listen_and_recognize(timeout=5.0)
        if text:
            print(f"è¯†åˆ«ç»“æœ: {text}")
        else:
            print("æœªè¯†åˆ«åˆ°è¯­éŸ³")
        
    except FileNotFoundError as e:
        print(f"é”™è¯¯: {e}")
        print("è¯·å…ˆä¸‹è½½ Vosk ä¸­æ–‡æ¨¡å‹:")
        print("wget https://alphacephei.com/vosk/models/vosk-model-cn-0.22.zip")
        print("unzip vosk-model-cn-0.22.zip")
    except Exception as e:
        print(f"è¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())


import numpy as np
import sounddevice as sd
import tempfile
import asyncio
import threading
import queue
import time
from typing import Optional, Callable
from faster_whisper import WhisperModel

class SpeechToText:
    """
    åŸºäº faster-whisper çš„è¯­éŸ³è½¬æ–‡å­—æœåŠ¡
    """
    
    def __init__(self, model_size: str = "base", language: str = "zh", device: str = "auto"):
        """
        åˆå§‹åŒ– Whisper STT æœåŠ¡
        
        Args:
            model_size: Whisper æ¨¡å‹å¤§å° (tiny, base, small, medium, large, large-v2, large-v3)
            language: è¯­è¨€ä»£ç  (zh, en, auto)
            device: è®¾å¤‡ç±»å‹ (cpu, cuda, auto)
        """
        self.language = language
        self.sample_rate = 16000
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.model_size = model_size
        
        # è°ƒè¯•ä¿¡æ¯ï¼šåˆå§‹åŒ–å¼€å§‹
        self.logger.info(f"ğŸš€ å¼€å§‹åˆå§‹åŒ– STT æœåŠ¡")
        self.logger.info(f"ğŸ“Š é…ç½®å‚æ•°: æ¨¡å‹={model_size}, è¯­è¨€={language}, è®¾å¤‡={device}")
        
        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        self.device = self._detect_device(device)
        self.compute_type = self._get_compute_type()
        
        # åˆå§‹åŒ– Whisper æ¨¡å‹
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹: {model_size}")
            self.logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}, è®¡ç®—ç±»å‹: {self.compute_type}")
            
            self.model = WhisperModel(
                model_size, 
                device=self.device, 
                compute_type=self.compute_type,
                download_root=os.path.expanduser("~/.cache/whisper")
            )
            
            load_time = time.time() - start_time
            self.logger.info(f"âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
            
        except Exception as e:
            self.logger.error(f"âŒ Whisper åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
            raise
        
        # éŸ³é¢‘é…ç½®
        self.chunk_size = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        
        # è¯­éŸ³æ£€æµ‹å‚æ•°
        self.energy_threshold = 300
        self.pause_threshold = 1.0
        self.phrase_threshold = 0.3
        self.min_audio_length = 0.5  # æœ€å°éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_recognitions': 0,
            'successful_recognitions': 0,
            'failed_recognitions': 0,
            'total_audio_time': 0.0,
            'total_processing_time': 0.0
        }
        
        self.logger.info(f"ğŸ¯ STT æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        self._log_system_info()
    
    def _detect_device(self, device: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡"""
        if device == "auto":
            try:
                import torch
                if torch.cuda.is_available():
                    self.logger.info(f"ğŸš€ æ£€æµ‹åˆ° CUDA æ”¯æŒï¼ŒGPU: {torch.cuda.get_device_name()}")
                    return "cuda"
                else:
                    self.logger.info(f"ğŸ’» ä½¿ç”¨ CPU è®¾å¤‡")
                    return "cpu"
            except ImportError:
                self.logger.info(f"ğŸ’» PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨ CPU è®¾å¤‡")
                return "cpu"
        else:
            self.logger.info(f"ğŸ¯ æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡: {device}")
            return device
    
    def _get_compute_type(self) -> str:
        """æ ¹æ®è®¾å¤‡è·å–æœ€ä½³è®¡ç®—ç±»å‹"""
        if self.device == "cuda":
            return "float16"
        else:
            return "int8"
    
    def _log_system_info(self):
        """è®°å½•ç³»ç»Ÿä¿¡æ¯"""
        try:
            import psutil
            cpu_count = psutil.cpu_count()
            memory = psutil.virtual_memory()
            self.logger.info(f"ğŸ’¾ ç³»ç»Ÿä¿¡æ¯: CPUæ ¸å¿ƒ={cpu_count}, å†…å­˜={memory.total//1024//1024//1024}GB")
        except ImportError:
            self.logger.debug("psutil æœªå®‰è£…ï¼Œè·³è¿‡ç³»ç»Ÿä¿¡æ¯è®°å½•")
    
    def _calculate_audio_volume(self, audio_data: bytes) -> float:
        """è®¡ç®—éŸ³é¢‘éŸ³é‡"""
        try:
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            if len(audio_array) == 0:
                return 0.0
            return float(np.sqrt(np.mean(audio_array.astype(np.float32)**2)))
        except Exception as e:
            self.logger.warning(f"âš ï¸ éŸ³é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _save_debug_audio(self, audio_data: bytes, prefix: str = "debug") -> str:
        """ä¿å­˜è°ƒè¯•éŸ³é¢‘æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}_{timestamp}.wav"
            filepath = os.path.join(tempfile.gettempdir(), filename)
            
            with wave.open(filepath, 'wb') as wf:
                wf.setnchannels(self.channels)
                wf.setsampwidth(2)  # 16-bit
                wf.setframerate(self.sample_rate)
                wf.writeframes(audio_data)
            
            self.logger.debug(f"ğŸµ è°ƒè¯•éŸ³é¢‘å·²ä¿å­˜: {filepath}")
            return filepath
        except Exception as e:
            self.logger.warning(f"âš ï¸ ä¿å­˜è°ƒè¯•éŸ³é¢‘å¤±è´¥: {e}")
            return ""
    
    async def listen_and_recognize(self, timeout: float = 5.0, phrase_time_limit: float = 10.0, 
                                 save_debug_audio: bool = False) -> Optional[str]:
        """
        ç›‘å¬éº¦å…‹é£å¹¶è¯†åˆ«è¯­éŸ³
        
        Args:
            timeout: ç­‰å¾…è¯­éŸ³å¼€å§‹çš„è¶…æ—¶æ—¶é—´
            phrase_time_limit: å•æ¬¡å½•éŸ³çš„æœ€å¤§æ—¶é•¿
            save_debug_audio: æ˜¯å¦ä¿å­˜è°ƒè¯•éŸ³é¢‘
        
        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        recognition_id = self.stats['total_recognitions'] + 1
        self.logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« #{recognition_id}")
        self.logger.debug(f"ğŸ“‹ å‚æ•°: timeout={timeout}s, phrase_limit={phrase_time_limit}s")
        
        start_time = time.time()
        
        try:
            def _record_audio():
                """å½•åˆ¶éŸ³é¢‘çš„åŒæ­¥å‡½æ•°"""
                self.logger.debug(f"ğŸ”§ åˆå§‹åŒ–éŸ³é¢‘è®¾å¤‡...")
                p = pyaudio.PyAudio()
                
                # åˆ—å‡ºå¯ç”¨çš„éŸ³é¢‘è®¾å¤‡
                self._log_audio_devices(p)
                
                stream = p.open(
                    format=self.format,
                    channels=self.channels,
                    rate=self.sample_rate,
                    input=True,
                    frames_per_buffer=self.chunk_size
                )
                
                frames = []
                recording = False
                silence_count = 0
                max_silence = int(self.sample_rate / self.chunk_size * self.pause_threshold)
                max_frames = int(self.sample_rate / self.chunk_size * phrase_time_limit)
                
                self.logger.debug(f"ğŸ¯ å¼€å§‹ç›‘å¬ï¼Œæœ€å¤§å¸§æ•°: {max_frames}, æœ€å¤§é™éŸ³: {max_silence}")
                
                try:
                    for i in range(max_frames):
                        data = stream.read(self.chunk_size, exception_on_overflow=False)
                        
                        # è®¡ç®—éŸ³é‡
                        volume = self._calculate_audio_volume(data)
                        
                        if i % 50 == 0:  # æ¯50å¸§è®°å½•ä¸€æ¬¡éŸ³é‡
                            self.logger.debug(f"ğŸ”Š éŸ³é‡: {volume:.2f}, é˜ˆå€¼: {self.energy_threshold}")
                        
                        if volume > self.energy_threshold:
                            if not recording:
                                self.logger.info(f"ğŸ™ï¸ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ï¼ŒéŸ³é‡: {volume:.2f}")
                                recording = True
                            frames.append(data)
                            silence_count = 0
                        elif recording:
                            frames.append(data)
                            silence_count += 1
                            
                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å½•éŸ³
                            if silence_count > max_silence:
                                silence_duration = silence_count * self.chunk_size / self.sample_rate
                                self.logger.info(f"ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³ {silence_duration:.2f}sï¼Œåœæ­¢å½•éŸ³")
                                break
                    
                    if frames:
                        # åˆå¹¶éŸ³é¢‘æ•°æ®
                        audio_data = b''.join(frames)
                        audio_duration = len(frames) * self.chunk_size / self.sample_rate
                        self.logger.info(f"ğŸ“Š å½•éŸ³å®Œæˆï¼Œæ—¶é•¿: {audio_duration:.2f}s, å¸§æ•°: {len(frames)}")
                        
                        # æ£€æŸ¥æœ€å°éŸ³é¢‘é•¿åº¦
                        if audio_duration < self.min_audio_length:
                            self.logger.warning(f"âš ï¸ éŸ³é¢‘æ—¶é•¿è¿‡çŸ­: {audio_duration:.2f}s < {self.min_audio_length}s")
                            return None
                        
                        return audio_data
                    else:
                        self.logger.warning(f"âš ï¸ æœªå½•åˆ¶åˆ°ä»»ä½•éŸ³é¢‘")
                        return None
                    
                finally:
                    stream.stop_stream()
                    stream.close()
                    p.terminate()
                    self.logger.debug(f"ğŸ”§ éŸ³é¢‘è®¾å¤‡å·²é‡Šæ”¾")
            
            # åœ¨çº¿ç¨‹ä¸­å½•åˆ¶éŸ³é¢‘
            self.logger.debug(f"ğŸ”„ å¼€å§‹å½•éŸ³çº¿ç¨‹...")
            audio_data = await asyncio.to_thread(_record_audio)
            
            if audio_data:
                # ä¿å­˜è°ƒè¯•éŸ³é¢‘
                if save_debug_audio:
                    self._save_debug_audio(audio_data, f"recognition_{recognition_id}")
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                self.logger.debug(f"ğŸ”„ è½¬æ¢éŸ³é¢‘æ ¼å¼...")
                audio_array = np.frombuffer(audio_data, dtype=np.int16)
                audio_float = audio_array.astype(np.float32) / 32768.0
                
                # ä½¿ç”¨Whisperè¯†åˆ«
                self.logger.info(f"ğŸ§  å¼€å§‹ Whisper è¯†åˆ«...")
                transcribe_start = time.time()
                
                segments, info = self.model.transcribe(
                    audio_float,
                    language=self.language if self.language != "auto" else None,
                    condition_on_previous_text=False,
                    vad_filter=True,
                    vad_parameters=dict(min_silence_duration_ms=500)
                )
                
                transcribe_time = time.time() - transcribe_start
                self.logger.info(f"âš¡ Whisper è¯†åˆ«å®Œæˆï¼Œè€—æ—¶: {transcribe_time:.2f}s")
                self.logger.debug(f"ğŸ“Š è¯†åˆ«ä¿¡æ¯: è¯­è¨€={info.language}, æ¦‚ç‡={info.language_probability:.3f}")
                
                # æå–æ–‡æœ¬
                text_parts = []
                for i, segment in enumerate(segments):
                    text = segment.text.strip()
                    if text:
                        self.logger.debug(f"ğŸ“ ç‰‡æ®µ {i+1}: [{segment.start:.2f}s-{segment.end:.2f}s] {text}")
                        text_parts.append(text)
                
                result = ' '.join(text_parts).strip()
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                total_time = time.time() - start_time
                audio_duration = len(audio_data) / (self.sample_rate * 2)  # 16-bit = 2 bytes
                
                self.stats['total_recognitions'] += 1
                self.stats['total_audio_time'] += audio_duration
                self.stats['total_processing_time'] += total_time
                
                if result:
                    self.stats['successful_recognitions'] += 1
                    self.logger.info(f"âœ… è¯†åˆ«æˆåŠŸ #{recognition_id}: '{result}'")
                    self.logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.2f}s, éŸ³é¢‘: {audio_duration:.2f}s, å¤„ç†: {transcribe_time:.2f}s")
                    return result
                else:
                    self.stats['failed_recognitions'] += 1
                    self.logger.warning(f"âŒ è¯†åˆ«å¤±è´¥ #{recognition_id}: æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")
                    return None
            else:
                self.stats['total_recognitions'] += 1
                self.stats['failed_recognitions'] += 1
                self.logger.warning(f"âŒ è¯†åˆ«å¤±è´¥ #{recognition_id}: æœªå½•åˆ¶åˆ°éŸ³é¢‘")
                return None
                
        except Exception as e:
            self.stats['total_recognitions'] += 1
            self.stats['failed_recognitions'] += 1
            total_time = time.time() - start_time
            self.logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¼‚å¸¸ #{recognition_id}: {e}")
            self.logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}, è€—æ—¶: {total_time:.2f}s")
            return None
    
    def _log_audio_devices(self, p: pyaudio.PyAudio):
        """è®°å½•å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡"""
        try:
            device_count = p.get_device_count()
            self.logger.debug(f"ğŸ§ æ£€æµ‹åˆ° {device_count} ä¸ªéŸ³é¢‘è®¾å¤‡:")
            
            for i in range(device_count):
                info = p.get_device_info_by_index(i)
                if info['maxInputChannels'] > 0:
                    self.logger.debug(f"  ğŸ“± è®¾å¤‡ {i}: {info['name']} (è¾“å…¥é€šé“: {info['maxInputChannels']})")
        except Exception as e:
            self.logger.debug(f"âš ï¸ è·å–éŸ³é¢‘è®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
    
    async def recognize_file(self, file_path: str) -> Optional[str]:
        """
        è¯†åˆ«éŸ³é¢‘æ–‡ä»¶ä¸­çš„è¯­éŸ³
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
        Returns:
            str: è¯†åˆ«çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        recognition_id = self.stats['total_recognitions'] + 1
        self.logger.info(f"ğŸ“ å¼€å§‹æ–‡ä»¶è¯†åˆ« #{recognition_id}: {file_path}")
        
        if not os.path.exists(file_path):
            self.logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        file_size = os.path.getsize(file_path)
        self.logger.debug(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
        
        start_time = time.time()
        
        try:
            self.logger.debug(f"ğŸ”„ å¼€å§‹è½¬å½•æ–‡ä»¶...")
            segments, info = self.model.transcribe(
                file_path,
                language=self.language if self.language != "auto" else None,
                vad_filter=True
            )
            
            transcribe_time = time.time() - start_time
            self.logger.info(f"âš¡ æ–‡ä»¶è½¬å½•å®Œæˆï¼Œè€—æ—¶: {transcribe_time:.2f}s")
            self.logger.debug(f"ğŸ“Š è¯†åˆ«ä¿¡æ¯: è¯­è¨€={info.language}, æ¦‚ç‡={info.language_probability:.3f}")
            
            text_parts = []
            total_duration = 0
            
            for i, segment in enumerate(segments):
                text = segment.text.strip()
                if text:
                    self.logger.debug(f"ğŸ“ ç‰‡æ®µ {i+1}: [{segment.start:.2f}s-{segment.end:.2f}s] {text}")
                    text_parts.append(text)
                    total_duration = max(total_duration, segment.end)
            
            result = ' '.join(text_parts).strip()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['total_recognitions'] += 1
            self.stats['total_audio_time'] += total_duration
            self.stats['total_processing_time'] += transcribe_time
            
            if result:
                self.stats['successful_recognitions'] += 1
                self.logger.info(f"âœ… æ–‡ä»¶è¯†åˆ«æˆåŠŸ #{recognition_id}: '{result}'")
                self.logger.info(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {total_duration:.2f}s, å¤„ç†è€—æ—¶: {transcribe_time:.2f}s")
                return result
            else:
                self.stats['failed_recognitions'] += 1
                self.logger.warning(f"âŒ æ–‡ä»¶è¯†åˆ«å¤±è´¥ #{recognition_id}: æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡æœ¬")
                return None
                
        except Exception as e:
            self.stats['total_recognitions'] += 1
            self.stats['failed_recognitions'] += 1
            processing_time = time.time() - start_time
            self.logger.error(f"âŒ æ–‡ä»¶è¯†åˆ«å¼‚å¸¸ #{recognition_id}: {e}")
            self.logger.error(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}, è€—æ—¶: {processing_time:.2f}s")
            return None
    
    async def record_audio(self, duration: float = 5.0, save_path: Optional[str] = None) -> str:
        """
        å½•åˆ¶éŸ³é¢‘åˆ°æ–‡ä»¶
        
        Args:
            duration: å½•åˆ¶æ—¶é•¿ï¼ˆç§’ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
        
        Returns:
            str: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(tempfile.gettempdir(), f"recording_{timestamp}.wav")
        
        self.logger.info(f"ğŸ™ï¸ å¼€å§‹å½•éŸ³ï¼Œæ—¶é•¿: {duration}s, ä¿å­˜åˆ°: {save_path}")
        
        def _record():
            p = pyaudio.PyAudio()
            
            stream = p.open(
                format=self.format,
                channels=self.channels,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size
            )
            
            frames = []
            frames_to_record = int(self.sample_rate / self.chunk_size * duration)
            
            self.logger.debug(f"ğŸ”„ å½•åˆ¶ {frames_to_record} å¸§...")
            
            for i in range(frames_to_record):
                data = stream.read(self.chunk_size, exception_on_overflow=False)
                frames.append(data)
                
                if i % 50 == 0:  # æ¯50å¸§è®°å½•ä¸€æ¬¡è¿›åº¦
                    progress = (i + 1) / frames_to_record * 100
                    self.logger.debug(f"ğŸ“Š å½•åˆ¶è¿›åº¦: {progress:.1f}%")
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            with wave.open(save_path, 'wb') as wf:
                wf.setnchannels(self.channels)
                wf.setsampwidth(p.get_sample_size(self.format))
                wf.setframerate(self.sample_rate)
                wf.writeframes(b''.join(frames))
            
            return save_path
        
        try:
            file_path = await asyncio.to_thread(_record)
            file_size = os.path.getsize(file_path)
            self.logger.info(f"âœ… å½•éŸ³å®Œæˆ: {file_path}, å¤§å°: {file_size / 1024:.1f} KB")
            return file_path
        except Exception as e:
            self.logger.error(f"âŒ å½•éŸ³å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats['total_recognitions'] > 0:
            stats['success_rate'] = stats['successful_recognitions'] / stats['total_recognitions'] * 100
            stats['avg_processing_time'] = stats['total_processing_time'] / stats['total_recognitions']
        else:
            stats['success_rate'] = 0.0
            stats['avg_processing_time'] = 0.0
        
        return stats
    
    def log_stats(self):
        """è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        self.logger.info(f"ğŸ“Š STT ç»Ÿè®¡ä¿¡æ¯:")
        self.logger.info(f"   æ€»è¯†åˆ«æ¬¡æ•°: {stats['total_recognitions']}")
        self.logger.info(f"   æˆåŠŸæ¬¡æ•°: {stats['successful_recognitions']}")
        self.logger.info(f"   å¤±è´¥æ¬¡æ•°: {stats['failed_recognitions']}")
        self.logger.info(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        self.logger.info(f"   æ€»éŸ³é¢‘æ—¶é•¿: {stats['total_audio_time']:.1f}s")
        self.logger.info(f"   æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.1f}s")
        self.logger.info(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.2f}s")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œè®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            if hasattr(self, 'stats') and self.stats['total_recognitions'] > 0:
                self.logger.info(f"ğŸ STT æœåŠ¡ç»“æŸ")
                self.log_stats()
        except:
            pass

# å…¼å®¹æ€§åˆ«å
class STT(SpeechToText):
    """å‘åå…¼å®¹çš„åˆ«å"""
    pass

# æµ‹è¯•å‡½æ•°
async def test_stt():
    """æµ‹è¯•STTåŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹ STT æµ‹è¯•...")
    
    try:
        # åˆå§‹åŒ–æœåŠ¡
        # å°†æ¨¡å‹ä» tiny å‡çº§åˆ° base æˆ– small
        stt_service = SpeechToText(model_size="base", language="zh")
        # æˆ–è€…ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹è·å¾—æ›´å¥½æ•ˆæœ
        # stt_service = SpeechToText(model_size="small", language="zh")
        print("\nğŸ¤ è¯·è¯´è¯ï¼ˆ5ç§’å†…ï¼‰...")
        result = await stt_service.listen_and_recognize(
            timeout=5.0, 
            phrase_time_limit=5.0,
            save_debug_audio=True
        )
        
        if result:
            print(f"âœ… è¯†åˆ«ç»“æœ: {result}")
        else:
            print("âŒ æœªè¯†åˆ«åˆ°è¯­éŸ³")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        stt_service.log_stats()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_stt())